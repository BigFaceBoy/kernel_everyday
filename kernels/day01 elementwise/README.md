# elementwise_add_f32_kernel
基本上所有的cuda教程都会介绍的vector add
# elementwise_add_f32x4_kernel
首先确定向量化是啥意思，f32x4 就是指每个线程处理4个连续的f32元素。  
可通过cuda 内置的 float4 来取值。
于是我写了第一个版本:
```
__global__ void elementwise_add_f32x4_kernel_bug(float *a, float *b, float *c, int N){
    int idx = 4 * (blockIdx.x * blockDim.x + threadIdx.x);
    if(idx < N){
        float4 reg_a = (reinterpret_cast<float4*>(a + idx))[0];
        float4 reg_b = (reinterpret_cast<float4*>(b + idx))[0];
        float4 reg_c;
        reg_c.x = reg_a.x + reg_b.x;
        reg_c.y = reg_a.y + reg_b.y;
        reg_c.z = reg_a.z + reg_b.z;
        reg_c.w = reg_a.w + reg_b.w;
        c[idx] = reinterpret_cast<float*>(&reg_c)[0];
    }
```
对比后意识到我最后只写回x分量，y、z、w被丢弃了。于是修改后有：
```
__global__ void elementwise_add_f32x4_kernel(float *a, float *b, float *c, int N){
    int idx = 4 * (blockIdx.x * blockDim.x + threadIdx.x);
    if(idx < N){
        float4 reg_a = (reinterpret_cast<float4*>(a + idx))[0];
        float4 reg_b = (reinterpret_cast<float4*>(b + idx))[0];
        float4 reg_c;
        reg_c.x = reg_a.x + reg_b.x;
        reg_c.y = reg_a.y + reg_b.y;
        reg_c.z = reg_a.z + reg_b.z;
        reg_c.w = reg_a.w + reg_b.w;
        (reinterpret_cast<float4*>(c + idx))[0] = reg_c;
    }
}
```
但是我在做reinterpret_cast时想到，为什么不直接用指针呢？于是有：
```
__global__ void elementwise_add_f32x4_ptr_kernel(float *a, float *b, float *c, int N){
    int idx = 4 * (blockIdx.x * blockDim.x + threadIdx.x);
    if(idx < N){
        float4* reg_a = reinterpret_cast<float4*>(a + idx);
        float4* reg_b = reinterpret_cast<float4*>(b + idx);
        float4* reg_c = reinterpret_cast<float4*>(c + idx);
        reg_c->x = reg_a->x + reg_b->x;
        reg_c->y = reg_a->y + reg_b->y;
        reg_c->z = reg_a->z + reg_b->z;
        reg_c->w = reg_a->w + reg_b->w;
    }
}
```
因为减少中间寄存器拷贝，我觉得指针版本性能会更快。于是我便用 [elementwise_benchmark](https://github.com/xlite-dev/LeetCUDA/blob/main/kernels/elementwise/elementwise.py) 测试了一下性能，结果如下：
```
-------------------------------------------------------------------------------------
                                        S=1024, K=1024
           out_f32: [2.61102319, 0.45886952], time:0.00458479ms
         out_f32x4: [2.61102319, 0.45886952], time:0.00396442ms
     out_f32x4_ptr: [2.61102319, 0.45886952], time:0.00936127ms
        out_f32_th: [2.61102319, 0.45886952], time:0.00527096ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=1024, K=2048
           out_f32: [-0.72178191, -2.10319948], time:0.00628018ms
         out_f32x4: [-0.72178191, -2.10319948], time:0.00591660ms
     out_f32x4_ptr: [-0.72178191, -2.10319948], time:0.01690555ms
        out_f32_th: [-0.72178191, -2.10319948], time:0.00572896ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=1024, K=4096
           out_f32: [1.89420867, 0.24196774], time:0.01066113ms
         out_f32x4: [1.89420867, 0.24196774], time:0.00973058ms
     out_f32x4_ptr: [1.89420867, 0.24196774], time:0.03196359ms
        out_f32_th: [1.89420867, 0.24196774], time:0.00971889ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=2048, K=1024
           out_f32: [1.29528499, 1.37868989], time:0.00743246ms
         out_f32x4: [1.29528499, 1.37868989], time:0.00590825ms
     out_f32x4_ptr: [1.29528499, 1.37868989], time:0.01720953ms
        out_f32_th: [1.29528499, 1.37868989], time:0.00572896ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=2048, K=2048
           out_f32: [-2.6689539, 1.04463267], time:0.01065230ms
         out_f32x4: [-2.6689539, 1.04463267], time:0.00979781ms
     out_f32x4_ptr: [-2.6689539, 1.04463267], time:0.03162980ms
        out_f32_th: [-2.6689539, 1.04463267], time:0.00963616ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=2048, K=4096
           out_f32: [-1.7215203, -0.5160538], time:0.05112147ms
         out_f32x4: [-1.7215203, -0.5160538], time:0.04940510ms
     out_f32x4_ptr: [-1.7215203, -0.5160538], time:0.05790949ms
        out_f32_th: [-1.7215203, -0.5160538], time:0.05045724ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=4096, K=1024
           out_f32: [2.03900576, 1.38476837], time:0.01309323ms
         out_f32x4: [2.03900576, 1.38476837], time:0.00980997ms
     out_f32x4_ptr: [2.03900576, 1.38476837], time:0.03195405ms
        out_f32_th: [2.03900576, 1.38476837], time:0.00969791ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=4096, K=2048
           out_f32: [1.23893905, -0.21672119], time:0.05234981ms
         out_f32x4: [1.23893905, -0.21672119], time:0.04912329ms
     out_f32x4_ptr: [1.23893905, -0.21672119], time:0.06574273ms
        out_f32_th: [1.23893905, -0.21672119], time:0.04828453ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=4096, K=4096
           out_f32: [-0.34702206, 3.06625533], time:0.28874302ms
         out_f32x4: [-0.34702206, 3.06625533], time:0.29398847ms
     out_f32x4_ptr: [-0.34702206, 3.06625533], time:0.28521490ms
        out_f32_th: [-0.34702206, 3.06625533], time:0.30064559ms
-------------------------------------------------------------------------------------
```
实际上指针版本更慢，我提了个[issue](https://github.com/xlite-dev/LeetCUDA/issues/341). 
总结大佬的分析，指针的写法相当于每次都去全局内存取数据写数据，而非指针的写法是先将数据拷贝到寄存器中，再从寄存器取数据。而全局内存的读写是相当昂贵的。

# elementwise_add_f16_kernel
数据类型为 half，有专门的 _hadd 操作
# elementwise_add_f16x2_kernel
有了 elementwise_add_f32x4_kernel 的参考，这个就比较快的写出来了
# elementwise_add_f16x8_kernel
同上，注意越界检查。其实上面的kernel都要写越界检查。
# elementwise_add_f16x8_pack_kernel
我不清楚 pack 的含义，所以就直接看大佬是怎么实现的了。
既然 fp16x8就是一次load 128 bits, 也就是用 float4.
那么fp16x2的向量化是否可以改写为load 32 bits, 也就是用 float2.
于是我改写了 `elementwise_add_f16x2_kernel`
```
#define FLOAT2(value) (reinterpret_cast<float2 *>(&(value))[0])
#define LDST32BITS(value) (reinterpret_cast<float2 *>(&(value))[0])
__global__ void elementwise_add_f16x2_pack_kernel(half *a, half *b, half *c, int N){
    int idx = 2 * (blockIdx.x * blockDim.x + threadIdx.x);
    if(idx < N){
        half pack_a[2],pack_b[2],pack_c[2];
        LDST32BITS(pack_a[0]) = LDST32BITS(a[idx]); // load 32 bits
        LDST32BITS(pack_b[0]) = LDST32BITS(b[idx]); // load 32 bits

        HALF2(pack_c[0]) = __hadd2(HALF2(pack_a[0]), HALF2(pack_b[0]));        
        // reinterpret as float4 and store 32 bits in 1 memory issue.
        if ((idx + 2) < N) {
            LDST32BITS(c[idx]) = LDST32BITS(pack_c[0]);
        }
    }
}
```
同样测了一下性能：
```
-------------------------------------------------------------------------------------
                                        S=1024, K=1024
-------------------------------------------------------------------------------------
           out_f16: [-0.02758789, -0.94726562], time:0.00417185ms
         out_f16x2: [-0.02758789, -0.94726562], time:0.00307441ms
    out_f16x2_pack: [-0.02758789, -0.94726562], time:0.00282145ms
         out_f16x8: [-0.02758789, -0.94726562], time:0.00304675ms
     out_f16x8pack: [-0.02758789, -0.94726562], time:0.00263548ms
        out_f16_th: [-0.02758789, -0.94726562], time:0.00479555ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=1024, K=2048
-------------------------------------------------------------------------------------
           out_f16: [-1.50390625, -0.13818359], time:0.00616527ms
         out_f16x2: [-1.50390625, -0.13818359], time:0.00461769ms
    out_f16x2_pack: [-1.50390625, -0.13818359], time:0.00469613ms
         out_f16x8: [-1.50390625, -0.13818359], time:0.00574255ms
     out_f16x8pack: [-1.50390625, -0.13818359], time:0.00394750ms
        out_f16_th: [-1.50390625, -0.13818359], time:0.00479817ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=1024, K=4096
-------------------------------------------------------------------------------------
           out_f16: [1.3046875, -0.296875], time:0.01052499ms
         out_f16x2: [1.3046875, -0.296875], time:0.01037502ms
    out_f16x2_pack: [1.3046875, -0.296875], time:0.01038098ms
         out_f16x8: [1.3046875, -0.296875], time:0.01050496ms
     out_f16x8pack: [1.3046875, -0.296875], time:0.00589776ms
        out_f16_th: [1.3046875, -0.296875], time:0.00607681ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=2048, K=1024
-------------------------------------------------------------------------------------
           out_f16: [0.55224609, 2.2265625], time:0.00678182ms
         out_f16x2: [0.55224609, 2.2265625], time:0.00398850ms
    out_f16x2_pack: [0.55224609, 2.2265625], time:0.00399661ms
         out_f16x8: [0.55224609, 2.2265625], time:0.00554705ms
     out_f16x8pack: [0.55224609, 2.2265625], time:0.00378656ms
        out_f16_th: [0.55224609, 2.2265625], time:0.00479460ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=2048, K=2048
-------------------------------------------------------------------------------------
           out_f16: [2.08007812, -1.35546875], time:0.01052594ms
         out_f16x2: [2.08007812, -1.35546875], time:0.00749874ms
    out_f16x2_pack: [2.08007812, -1.35546875], time:0.00760913ms
         out_f16x8: [2.08007812, -1.35546875], time:0.00941133ms
     out_f16x8pack: [2.08007812, -1.35546875], time:0.00587749ms
        out_f16_th: [2.08007812, -1.35546875], time:0.00606537ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=2048, K=4096
-------------------------------------------------------------------------------------
           out_f16: [-1.41601562, -0.76464844], time:0.01921320ms
         out_f16x2: [-1.41601562, -0.76464844], time:0.01898718ms
    out_f16x2_pack: [-1.41601562, -0.76464844], time:0.01898718ms
         out_f16x8: [-1.41601562, -0.76464844], time:0.02025914ms
     out_f16x8pack: [-1.41601562, -0.76464844], time:0.00977349ms
        out_f16_th: [-1.41601562, -0.76464844], time:0.01036310ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=4096, K=1024
-------------------------------------------------------------------------------------
           out_f16: [1.03710938, 2.48046875], time:0.01194906ms
         out_f16x2: [1.03710938, 2.48046875], time:0.00621772ms
    out_f16x2_pack: [1.03710938, 2.48046875], time:0.00622606ms
         out_f16x8: [1.03710938, 2.48046875], time:0.00917053ms
     out_f16x8pack: [1.03710938, 2.48046875], time:0.00569177ms
        out_f16_th: [1.03710938, 2.48046875], time:0.00606632ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=4096, K=2048
-------------------------------------------------------------------------------------
           out_f16: [1.6484375, -0.51318359], time:0.01917052ms
         out_f16x2: [1.6484375, -0.51318359], time:0.01331782ms
    out_f16x2_pack: [1.6484375, -0.51318359], time:0.01339197ms
         out_f16x8: [1.6484375, -0.51318359], time:0.01715326ms
     out_f16x8pack: [1.6484375, -0.51318359], time:0.00978398ms
        out_f16_th: [1.6484375, -0.51318359], time:0.01036000ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=4096, K=4096
-------------------------------------------------------------------------------------
           out_f16: [1.0234375, -0.08374023], time:0.04817390ms
         out_f16x2: [1.0234375, -0.08374023], time:0.04438066ms
    out_f16x2_pack: [1.0234375, -0.08374023], time:0.04440117ms
         out_f16x8: [1.0234375, -0.08374023], time:0.04169416ms
     out_f16x8pack: [1.0234375, -0.08374023], time:0.03717375ms
        out_f16_th: [1.0234375, -0.08374023], time:0.03733087ms
-------------------------------------------------------------------------------------
```
f16x2 和 f16x2_pack 性能相当。
